{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install \"pyautogen[gemini]~=0.2.0b4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pdb\n",
    "import os\n",
    "import re\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union\n",
    "\n",
    "import autogen\n",
    "from autogen import AssistantAgent, Agent, UserProxyAgent, ConversableAgent\n",
    "\n",
    "from autogen.agentchat.contrib.img_utils import get_image_data, _to_pil\n",
    "from autogen.agentchat.contrib.multimodal_conversable_agent import MultimodalConversableAgent\n",
    "\n",
    "from termcolor import colored\n",
    "import random\n",
    "\n",
    "from autogen.code_utils import DEFAULT_MODEL, UNKNOWN, content_str, execute_code, extract_code, infer_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gemini-pro\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = {\"config_list\": config_list, \"cache_seed\": 42}\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    system_message=\"A human admin.\",\n",
    "    code_execution_config={\"last_n_messages\": 3, \"work_dir\": \"groupchat\"},\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "coder = autogen.AssistantAgent(\n",
    "    name=\"Coder\",\n",
    "    system_message=\"\"\"You are an expert in extracting product quantity details from titles. Given a product title, identify relevant details and output them in the following JSON format with appropriate field labels:\n",
    "                    {'unit_of_measure': , 'count_per_pack': , 'multi_quantity':, 'total_quantity':}\n",
    "                    Guidelines:\n",
    "                    If count per pack is not given in the provided value and multi quantity is given, set count per pack to 1; if multi quantity is not given and count per pack is given, set multi quantity to 1.\n",
    "                    \"\"\",\n",
    "    llm_config=llm_config,\n",
    "    max_consecutive_auto_reply=1\n",
    ")\n",
    "pm = autogen.AssistantAgent(\n",
    "    name=\"Product_manager\",\n",
    "    system_message=\"\"\"Calculate total quantity is equal to count per pack * multi quantity\"\"\",\n",
    "    llm_config=llm_config,\n",
    "    max_consecutive_auto_reply=1\n",
    ")\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, coder, pm], messages=[], max_round=10,allow_repeat_speaker=False)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"V8 +Energy Lightly Carbonated, Blackberry Cranberry, 12 Ounce, 4 Count\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "V8 +Energy Lightly Carbonated, Blackberry Cranberry, 12 Ounce, 4 Count\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model gemini-pro not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model gemini-pro not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "```\n",
      "{\n",
      "  \"unit_of_measure\": \"Ounce\",\n",
      "  \"count_per_pack\": 4,\n",
      "  \"multi_quantity\": 1,\n",
      "  \"total_quantity\": 48\n",
      "}\n",
      "```\n",
      "\n",
      "The total quantity is calculated by multiplying the count per pack by the multi-quantity.\n",
      "\n",
      "Count per pack = 4\n",
      "Multi quantity = 1\n",
      "\n",
      "Total quantity = Count per pack * Multi quantity\n",
      "Total quantity = 4 * 1\n",
      "Total quantity = 4\n",
      "\n",
      "Since the unit of measure is \"Ounce\", the total quantity is 48 Ounces.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model gemini-pro not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    }
   ],
   "source": [
    "results = user_proxy.initiate_chat(\n",
    "    manager, message=task)\n",
    "# type exit to terminate the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_proxy.send(\n",
    "#     recipient=manager,\n",
    "#     message=\"\"\"\"\"\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
